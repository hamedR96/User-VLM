<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>User-VLM 360° </title>
  <link rel="icon" type="image/x-icon" href="static/images/6134346.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">User-VLM 360°</h1>
            <h1 class="title is-2 publication-title">Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions</h1>
             <img src="static/images/arch.png" width="100%" height="600px" > </img>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.hamedrahimi.fr" target="_blank">Hamed Rahimi</a><sup><b style="color:#008AD7; font-weight:normal">▶ </b></sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=sMZJfr0AAAAJ&hl=fr" target="_blank">Adil Bahaj</a><sup><b style="color:#f68946; font-weight:normal">▶ </b></sup>,</span>
                  <span class="author-block">
                    <a href="https://www.isir.upmc.fr/personnel/mouad-abrini/" target="_blank">Mouad Abrini</a><sup><b style="color:#008AD7; font-weight:normal">▶ </b></sup>,</span>
                    <span class="author-block">
                    <a href="https://www.isir.upmc.fr/personnel/khoramshahi/?lang=en" target="_blank">Mahdi Khoramshahi</a><sup><b style="color:#008AD7; font-weight:normal">▶ </b></sup>,</span>
                      <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=aIXHNpAAAAAJ&hl=en" target="_blank">Mounir Ghogho</a><sup><b style="color:#f68946; font-weight:normal">▶ </b></sup>,</span>
                        <span class="author-block">
                    <a href="https://www.isir.upmc.fr/personnel/chetouani/" target="_blank">Mohamed Chetouani</a><sup><b style="color:#008AD7; font-weight:normal">▶ </b></sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!--<span class="author-block">Institution Name<br>Conferance name and year</span>-->
                    <!--<span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>
                <div class="is-size-5 publication-authors">

              <span class="author-block"><b style="color:#008AD7; font-weight:normal">▶ </b>ISIR, Sorbonne University</span>
              <span class="author-block"><b style="color:#f68946; font-weight:normal">▶ </b>International University of Rabat</span>

            </div>
                   <!--
                  <div class="column has-text-centered">
                    <div class="publication-links">

                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/hamedR96/User-VLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>


                 <span class="link-block">
                  <a href="https://huggingface.co/ACIDE/User-VLM-10B-Instruct" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-robot"></i> <!-- Or another relevant icon -->
                    </span>
                    <span>Models</span>
                  </a>
                </span>



                      <span class="link-block">
                  <a href="https://huggingface.co/ACIDE" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-robot"></i> <!-- Or another relevant icon -->
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The integration of vision-language models into robotic systems constitutes a significant advancement in enabling machines to interact with their surroundings in a more intuitive manner. While VLMs offer rich multimodal reasoning, existing approaches lack user-specific adaptability, often relying on generic interaction paradigms that fail to account for individual behavioral, contextual, or socio-emotional nuances. When customization is attempted, ethical concerns arise from unmitigated biases in user data, risking exclusion or unfair treatment. To address these dual challenges, we propose User-VLM 360°, a holistic framework integrating multimodal user modeling with bias-aware optimization. Our approach features: (1) user-aware tuning that adapts interactions in real time using visual-linguistic signals; (2) bias mitigation via preference optimization; and (3) curated 360° socio-emotive interaction datasets annotated with demographic, emotion, and relational metadata. Evaluations across eight benchmarks demonstrate state-of-the-art results: +35.3% F1 in personalized VQA, +47.5% F1 in facial features understanding, 15% bias reduction, and 30× speedup over baselines. Ablation studies confirm component efficacy, and deployment on the Pepper robot validates real-time adaptability across diverse users. We open-source parameter-efficient 3B/10B models and an ethical verification framework for responsible adaptation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


  <!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Deployment on Pepper (videos coming soon...)</h2>
        <div class="content has-text-justified">
          <img src="static/images/pepper2.pdf" style="display: block; margin-left: auto; margin-right: auto;" width="80%" height="300px"> </img>
          <p>
            User-aware Tuning mitigates the semantic gap arising from the misalignment between user queries and the observed scene as captured from the robot's camera perspective. While instruction-tuning could address this for large VLMs, it adds latency and reduces performance. User-VLM 360° overcomes this by natively aligning cross-modal representations, enabling robust real-time adaptation in dynamic robotic environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
     <!-- <img training="static/images/arch.png" width="100%" height="600px" > </img>  -->
      <h2 class="subtitle has-text-centered">
        User-aware Tuning consists of three key steps: In the first step, Vision Alignment, the model is trained to recognize and interpret human emotions, age, gender, and ethnicity based on facial features and visual signals.
        In the second step, Instruction Tuning, the model undergoes supervised instruction tuning, enabling it to respond effectively to general-purpose questions by incorporating visual cues. Finally, to mitigate over-personalization and prevent biased or unethical responses, the third step, Bias Mitigation, focuses on training the model to generate ethical and contextually appropriate responses.
      </h2>
    </div>
  </div>
</section>





<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Result</h2>
      <div id="results-carousel-1" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/result.png" alt="MY ALT TEXT" style="display: block; margin-left: auto; margin-right: auto;">

        <h2 class="subtitle" style="text-align: left;">
          User-VLM 360° consistently outperforms baseline models across multiple benchmarks, demonstrating superior user-aware personalization, facial feature understanding, and general-purpose reasoning. It achieves up to a 2x improvement in ROUGE-1 F1 scores over baselines in user-centric VQA tasks, surpasses all baselines in facial recognition tasks, and establishes a new state-of-the-art in key areas such as race detection, face counting, and emotion detection. Despite being trained on user-related data, it maintains strong generalization, matching or exceeding baseline performance on VQAv2 and COCO while exhibiting only minor gaps in unstructured data scenarios.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/bias.png" alt="MY ALT TEXT" style="display: block; margin-left: auto; margin-right: auto;">

        <h2 class="subtitle" style="text-align: left;">
          User-VLM 360° demonstrates superior initial performance in terms of fairness compared to the baseline, as measured by ROUGE-1 and BERTScore. Following DPO tuning, the models generally exhibit improved performance on these metrics, further enhancing their safety and fairness profiles.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/performance.png" alt="MY ALT TEXT" style="display: block; margin-left: auto; margin-right: auto;">
        <h2 class="subtitle" style="text-align: left;">
         User-VLM 360° achieves a substantial reduction in computational complexity, measured in FLOPs, by eliminating the need for explicit instruction-based prompting. Specifically, assuming a question prompt of 50 tokens and detailed instructions of 100 tokens for general-purpose VLMs, the compact 3B variant of User-VLM 360° exhibits a remarkable 17.5–30X reduction in FLOPs compared to larger 7B–12B baseline models. Furthermore, even the 10B variant of User-VLM 360° outperforms equivalently sized models by a significant margin, achieving a 5.25–16.5X reduction in FLOPs.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
  <!--     <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">  -->
            <!-- Youtube embed code here -->
      <!--       <iframe training="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">-->
            <!-- Your video file here -->
         <!--   <source training="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
          <!--   <source training="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\  -->
            <!-- Your video file here -->
          <!--   <source training="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  training="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{rahimi2025user,
  title={User-VLM: LLM Contextualization with Multimodal Pre-trained User Models},
  author={Rahimi, Hamed and Abrini, Mouad and Khoramshahi, Mahdi and Chetouani, Mohamed},
  year={2025}
}

@article{rahimi2025user,
  title={User-VLM 360°: Personalized Vision Language Models with User-aware Tuning for Social Human Robot Interactions},
  author={Rahimi, Hamed and Bhaj, Adil, Abrini, Mouad, Khoramshahi, Mahdi, Ghogho, Mounir, and Chetouani, Mohamed},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
